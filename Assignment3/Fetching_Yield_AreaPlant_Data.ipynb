{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62abd29",
   "metadata": {},
   "source": [
    "## <span style=color:blue>This notebook is fetching WinterWheat yields for an ML pipeline </spann>\n",
    "\n",
    "<span style=color:blue>It pulls from USDA NASS.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "748db334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This useful if I want to give unique names to directories or files\n",
    "import datetime\n",
    "def curr_timestamp():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    return formatted_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bcb69",
   "metadata": {},
   "source": [
    "### <span style=color:blue> Accessing USDA NASS, following code from https://towardsdatascience.com/harvest-and-analyze-agricultural-data-with-the-usda-nass-api-python-and-tableau-a6af374b8138.  In first cell below we define a class for interacting with the NASS QuickStats API, and in second cell we illustrate how to invoke that class </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ec026614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/harvest-and-analyze-agricultural-data-with-the-usda-nass-api-python-and-tableau-a6af374b8138\n",
    "# with edits\n",
    "\n",
    "#   Name:           c_usda_quick_stats.py\n",
    "#   Author:         Randy Runtsch\n",
    "#   Date:           March 29, 2022\n",
    "#   Project:        Query USDA QuickStats API\n",
    "#   Author:         Randall P. Runtsch\n",
    "#\n",
    "#   Description:    Query the USDA QuickStats api_GET API with a specified set of \n",
    "#                   parameters. Write the retrieved data, in CSV format, to a file.\n",
    "#\n",
    "#   See Quick Stats (NASS) API user guide:  https://quickstats.nass.usda.gov/api\n",
    "#   Request a QuickStats API key here:      https://quickstats.nass.usda.gov/api#param_define\n",
    "#\n",
    "#   Attribution: This product uses the NASS API but is not endorsed or certified by NASS.\n",
    "#\n",
    "#   Changes\n",
    "#\n",
    "\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from requests.utils import requote_uri\n",
    "import requests\n",
    "\n",
    "# Retrieve NASS API key from environment variables (you have to get your own)\n",
    "import os\n",
    "#my_NASS_API_key = os.getenv('NASS_API_KEY')\n",
    "my_NASS_API_key = 'CECCBC6B-9398-356F-9BCC-C326CBB2DFB4'\n",
    "\n",
    "class c_usda_quick_stats:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Set the USDA QuickStats API key, API base URL, and output file path where CSV files will be written. \n",
    "\n",
    "        # self.api_key = 'PASTE_YOUR_API_KEY_HERE'\n",
    "        self.api_key = my_NASS_API_key\n",
    "\n",
    "        self.base_url_api_get = 'http://quickstats.nass.usda.gov/api/api_GET/?key=' \\\n",
    "                                + self.api_key + '&'\n",
    "\n",
    "    def get_data(self, parameters, file_path, file_name):\n",
    "\n",
    "        # Call the api_GET api with the specified parameters. \n",
    "        # Write the CSV data to the specified output file.\n",
    "\n",
    "        # Create the full URL and retrieve the data from the Quick Stats server.\n",
    "        \n",
    "        full_url = self.base_url_api_get + parameters        \n",
    "        print(full_url)\n",
    "\n",
    "        try:\n",
    "            s_result = urllib.request.urlopen(full_url)\n",
    "            # print(type(s_result))\n",
    "            print(s_result.status, s_result.reason)\n",
    "            # print(s_result.status_code)\n",
    "            s_text = s_result.read().decode('utf-8')\n",
    "\n",
    "            # Create the output file and write the CSV data records to the file.\n",
    "\n",
    "            s_file_name = file_path + file_name\n",
    "            o_file = open(s_file_name, \"w\", encoding=\"utf8\")\n",
    "            o_file.write(s_text)\n",
    "            o_file.close()\n",
    "        except HTTPError as error:\n",
    "            print(error.code, error.reason)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while fetching the data: {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to parse the response data: {e}\")\n",
    "        except:\n",
    "            print(f\"Failed because of unknown exception; perhaps the USDA NASS site is down\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1559bf",
   "metadata": {},
   "source": [
    "<span style=color:blue>First, a test query based on Randall Runtsch...    </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da48f5fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quickstats.nass.usda.gov/api/api_GET/?key=CECCBC6B-9398-356F-9BCC-C326CBB2DFB4&source_desc=SURVEY&sector_desc%3DFARMS%20%26%20LANDS%20%26%20ASSETS&commodity_desc%3DFARM%20OPERATIONS&statisticcat_desc%3DAREA%20OPERATED&unit_desc=ACRES&freq_desc=ANNUAL&reference_period_desc=YEAR&year__GE=1997&agg_level_desc=NATIONAL&state_name%3DUS%20TOTAL&format=CSV\n",
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "# from https://towardsdatascience.com/harvest-and-analyze-agricultural-data-with-the-usda-nass-api-python-and-tableau-a6af374b8138\n",
    "# with edits\n",
    "\n",
    "#   Date:           March 29, 2022\n",
    "#   Project:        Program controller to query USDA QuickStats API\n",
    "#   Author:         Randall P. Runtsch\n",
    "#\n",
    "#   Description:    Create an instance of the c_usda_quick_stats class. Call it with\n",
    "#                   the desired search parameter and output file name.\n",
    "#\n",
    "#   Attribution: This product uses the NASS API but is not endorsed or certified by NASS.\n",
    "#\n",
    "#   Changes\n",
    "#\n",
    "\n",
    "import sys\n",
    "import urllib.parse\n",
    "\n",
    "#output_dir = '/Users/rick/AG-CODE--v03/USDA-NASS--v01/OUTPUTS/'\n",
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/'\n",
    "\n",
    "# Create a string with search parameters, then create an instance of\n",
    "# the c_usda_quick_stats class and use that to fetch data from QuickStats\n",
    "# and write it to a file\n",
    "\n",
    "# the QuickStats site is very senstivite to how the full URL is built up.\n",
    "# For example, the following spec for the parameters works\n",
    "# But if you replace the line \"'&unit_desc=ACRES' + \\\" with\n",
    "# the line \"'&' + urllib.parse.quote('unit_desc-ACRES')\"\n",
    "# then the site responds saying that you have exceeded the 50,000 record limit for one query\n",
    "\n",
    "parameters =    'source_desc=SURVEY' +  \\\n",
    "                '&' + urllib.parse.quote('sector_desc=FARMS & LANDS & ASSETS') + \\\n",
    "                '&' + urllib.parse.quote('commodity_desc=FARM OPERATIONS') + \\\n",
    "                '&' + urllib.parse.quote('statisticcat_desc=AREA OPERATED') + \\\n",
    "                '&unit_desc=ACRES' + \\\n",
    "                '&freq_desc=ANNUAL' + \\\n",
    "                '&reference_period_desc=YEAR' + \\\n",
    "                '&year__GE=1997' + \\\n",
    "                '&agg_level_desc=NATIONAL' + \\\n",
    "                '&' + urllib.parse.quote('state_name=US TOTAL') + \\\n",
    "                '&format=CSV'\n",
    "\n",
    "stats = c_usda_quick_stats()\n",
    "\n",
    "# Including curr_timestamp() into file name to keep outputs separated during development/exploration\n",
    "s_json = stats.get_data(parameters, output_dir, 'national_farm_survey_acres_ge_1997_' + curr_timestamp() + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bf18e",
   "metadata": {},
   "source": [
    "<span style=color:blue>Now a query that fetches useful soybean yield data.  I am focused on the top Kansas states in the US, and on the years 2003 to 2022.   </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6510e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quickstats.nass.usda.gov/api/api_GET/?key=CECCBC6B-9398-356F-9BCC-C326CBB2DFB4&source_desc=SURVEY&sector_desc=CROPS&group_desc%3DFIELD%20CROPS&commodity_desc=WHEAT&statisticcat_desc=YIELD&geographic_level=STATE&agg_level_desc=COUNTY&state_name=KANSAS&year__GE=2003&year__LE=2023&format=CSV\n",
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib.parse\n",
    "\n",
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/USDA-NASS--v01/'\n",
    "\n",
    "# Create a string with search parameters, then create an instance of\n",
    "# the c_usda_quick_stats class and use that to fetch data from QuickStats\n",
    "# and write it to a file\n",
    "\n",
    "# It took a while to get the parameter names just right...\n",
    "#   The parameters names are listed in\n",
    "#      https://quickstats.nass.usda.gov/param_define\n",
    "#   (some additional resources in https://quickstats.nass.usda.gov/tutorials)\n",
    "#   Also, look at the column names that show up in the csv files that you get back\n",
    "parameters =    'source_desc=SURVEY' +  \\\n",
    "                '&sector_desc=CROPS' + \\\n",
    "                '&' + urllib.parse.quote('group_desc=FIELD CROPS') + \\\n",
    "                '&commodity_desc=WHEAT' + \\\n",
    "                '&statisticcat_desc=YIELD' + \\\n",
    "                '&geographic_level=STATE' + \\\n",
    "                '&agg_level_desc=COUNTY' + \\\n",
    "                '&state_name=KANSAS' + \\\n",
    "                '&year__GE=2003' + \\\n",
    "                '&year__LE=2023' + \\\n",
    "                '&format=CSV'\n",
    "\n",
    "stats = c_usda_quick_stats()\n",
    "\n",
    "# holding this timestamp; we may used it to import the created csv file\n",
    "latest_curr_timestamp = curr_timestamp()\n",
    "filename = 'wheat_yield_data__' + latest_curr_timestamp + '.csv'\n",
    "\n",
    "# Including curr_timestamp() into file name to keep outputs separated during development/exploration\n",
    "stats.get_data(parameters, output_dir, 'wheat_yield_data__' + latest_curr_timestamp + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c890336",
   "metadata": {},
   "source": [
    "### <span style=color:blue>After inspecting the output we see that there is double counting.  In particular, see the columns for \"short_desc\".  So, we will drop all records with short_desc != \"SOYBEANS - YIELD, MEASURED IN BU / ACRE\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dec3aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6068\n",
      "6068\n",
      "1893\n",
      "\n",
      "1817\n",
      "\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#output_dir = '/Users/rick/AG-CODE--v03/USDA-NASS--v01/OUTPUTS/'\n",
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/USDA-NASS--v01/'\n",
    "\n",
    "df = pd.read_csv(output_dir + filename)\n",
    "#print(df.head())\n",
    "print(len(df))\n",
    "\n",
    "df1 = df[['short_desc']].drop_duplicates()\n",
    "#print(df1.head(10))\n",
    "print(len(df))\n",
    "\n",
    "# keep only records about full yield\n",
    "#df = df[df['short_desc'].isin(['WHEAT - YIELD, MEASURED IN BU / ACRE', 'WHEAT, WINTER - YIELD, MEASURED IN BU / ACRE'])]\n",
    "\n",
    "## We decided to just select Winter Wheat. Because there are whole wheat yiled data just from 2003 ~ 2007\n",
    "df = df[df['short_desc'].isin(['WHEAT, WINTER - YIELD, MEASURED IN BU / ACRE'])]\n",
    "print(len(df))\n",
    "# 10295\n",
    "\n",
    "print()\n",
    "\n",
    "# found some bad_county_names by visual inspection of the csv\n",
    "bad_county_names = ['OTHER COUNTIES', 'OTHER (COMBINED) COUNTIES']\n",
    "df = df[~df.county_name.isin(bad_county_names)]\n",
    "\n",
    "print(len(df))\n",
    "# 9952\n",
    "\n",
    "print()\n",
    "\n",
    "df2 = df[['state_name','county_name']].drop_duplicates()\n",
    "print(len(df2))\n",
    "# 559\n",
    "\n",
    "# Note: using SQL I found that of the 559 state-county pairs total:\n",
    "#          212 state-county pairs have data for all 20 years\n",
    "#          347 state-county pairs have data for < 20 years\n",
    "#\n",
    "#          486 have year 2022\n",
    "#          418 have year 2021\n",
    "#          514 have year 2020\n",
    "# I will live with that\n",
    "\n",
    "# cleaning up a column name\n",
    "df = df.rename(columns={'Value': 'yield'})\n",
    "\n",
    "#output_dir = '/Users/rick/AG-CODE--v03/USDA-NASS--v01/OUTPUTS/'\n",
    "output_file = 'repaired_yield__' + curr_timestamp() + '.csv'\n",
    "\n",
    "df.to_csv(output_dir + output_file, index=False)\n",
    "\n",
    "# I imported this table into postgres so that I could use SQL ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a6b5c",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Data cleaning and wranggling </span>\n",
    "#### <span style=color:blue> Check if each county has yield data for each year    </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98f884e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALLEN': [2016, 2017, 2018, 2020, 2010, 2012], 'ANDERSON': [2010, 2022], 'ATCHISON': [2016, 2012], 'BARBER': [2016, 2017, 2018, 2019, 2015], 'BARTON': [], 'BOURBON': [2019], 'BROWN': [2018], 'BUTLER': [2017], 'CHASE': [2013], 'CHAUTAUQUA': [2019, 2014], 'CHEROKEE': [2017, 2010], 'CHEYENNE': [2016], 'CLARK': [2014], 'CLAY': [2019, 2015], 'CLOUD': [2016, 2018, 2019, 2015], 'COFFEY': [2008, 2010, 2016, 2014], 'COMANCHE': [2012], 'COWLEY': [2017, 2018], 'CRAWFORD': [2018, 2013, 2014, 2015], 'DECATUR': [2018], 'DICKINSON': [2019], 'DONIPHAN': [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015], 'DOUGLAS': [2016], 'EDWARDS': [2016], 'ELK': [2019, 2020, 2015], 'ELLIS': [2017, 2021], 'ELLSWORTH': [2016, 2013], 'FINNEY': [2016, 2017, 2018, 2019], 'FORD': [2019, 2021, 2022], 'FRANKLIN': [2008, 2013], 'GEARY': [2020], 'GOVE': [2017], 'GRAHAM': [2016, 2017, 2018, 2019, 2013, 2014, 2015], 'GRANT': [2008, 2018], 'GRAY': [2018, 2021], 'GREELEY': [2018], 'GREENWOOD': [2008, 2010, 2012], 'HAMILTON': [2018], 'HARPER': [2019], 'HARVEY': [], 'HASKELL': [2016, 2017, 2018, 2019, 2014, 2015], 'HODGEMAN': [2016, 2017, 2018, 2019, 2015], 'JACKSON': [2020, 2013], 'JEFFERSON': [2019, 2013, 2014], 'JEWELL': [2019, 2021], 'JOHNSON': [2018, 2021, 2022, 2008, 2015], 'KEARNY': [2018, 2019, 2015], 'KINGMAN': [2019], 'KIOWA': [2017, 2018, 2014, 2015], 'LABETTE': [2016, 2018], 'LANE': [2008, 2017, 2018], 'LEAVENWORTH': [], 'LINCOLN': [2017, 2018, 2019, 2021, 2015], 'LINN': [2017, 2010, 2020, 2013], 'LOGAN': [2017, 2019, 2022, 2013, 2015], 'LYON': [2018, 2014], 'MARION': [2021], 'MARSHALL': [2017, 2018, 2019, 2021, 2015], 'MCPHERSON': [], 'MEADE': [2017, 2018, 2019, 2021, 2022, 2014], 'MIAMI': [2018, 2020, 2022], 'MITCHELL': [], 'MONTGOMERY': [2008], 'MORRIS': [], 'MORTON': [2022], 'NEMAHA': [2018, 2021, 2011, 2013, 2014, 2015], 'NEOSHO': [2017, 2020], 'NESS': [2017, 2018], 'NORTON': [2016, 2018, 2019, 2021], 'OSAGE': [], 'OSBORNE': [2017, 2019, 2022], 'OTTAWA': [], 'PAWNEE': [], 'PHILLIPS': [2019], 'POTTAWATOMIE': [2020], 'PRATT': [2016, 2019, 2014], 'RAWLINS': [2018, 2022], 'RENO': [2021], 'REPUBLIC': [], 'RICE': [2018, 2019, 2014, 2015], 'RILEY': [2017, 2018, 2019, 2021, 2022, 2015], 'ROOKS': [2022, 2015], 'RUSH': [2016, 2017, 2018, 2019, 2022], 'RUSSELL': [2013, 2014], 'SALINE': [], 'SCOTT': [2016, 2019, 2021, 2014, 2015], 'SEDGWICK': [], 'SEWARD': [2013], 'SHAWNEE': [], 'SHERIDAN': [2017, 2018, 2019, 2021, 2022, 2013, 2015], 'SHERMAN': [], 'SMITH': [2019], 'STAFFORD': [2016, 2019, 2012, 2021], 'STANTON': [2016, 2018, 2019, 2020, 2022, 2013, 2014, 2015], 'STEVENS': [2008], 'SUMNER': [], 'THOMAS': [2016, 2017, 2018, 2019, 2021, 2022, 2013, 2014], 'TREGO': [2016, 2017, 2018, 2019, 2012], 'WABAUNSEE': [2017, 2018, 2021, 2022, 2015], 'WALLACE': [2008, 2019, 2012, 2013], 'WASHINGTON': [2016, 2017, 2018, 2019], 'WICHITA': [2014], 'WILSON': [2008, 2019], 'WOODSON': [2019, 2013, 2014], 'WYANDOTTE': [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]}\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# Group the data by 'county' and get the minimum and maximum years for each county\n",
    "county_years = df.groupby('county_name')['year'].agg(['min', 'max'])\n",
    "\n",
    "start_year = 2003\n",
    "end_year = 2022\n",
    "\n",
    "# Iterate over each county and find missing years for each county\n",
    "missing_years_by_county = {}\n",
    "for county, years in county_years.iterrows():\n",
    "\n",
    "    # Generate a list of all years between the start and end years for each county\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "\n",
    "    # Find any missing years for each county by comparing the list of all years with the available years in the data\n",
    "    missing_years_by_county[county] = list(set(all_years) - set(df[df['county_name'] == county]['year']))\n",
    "    \n",
    "\n",
    "print(missing_years_by_county)\n",
    "print(len(missing_years_by_county))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97930c4c",
   "metadata": {},
   "source": [
    "#### <span style=color:blue> Count omitted yield each counties</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "726f7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALLEN': 6, 'ANDERSON': 2, 'ATCHISON': 2, 'BARBER': 5, 'BARTON': 0, 'BOURBON': 1, 'BROWN': 1, 'BUTLER': 1, 'CHASE': 1, 'CHAUTAUQUA': 2, 'CHEROKEE': 2, 'CHEYENNE': 1, 'CLARK': 1, 'CLAY': 2, 'CLOUD': 4, 'COFFEY': 4, 'COMANCHE': 1, 'COWLEY': 2, 'CRAWFORD': 4, 'DECATUR': 1, 'DICKINSON': 1, 'DONIPHAN': 15, 'DOUGLAS': 1, 'EDWARDS': 1, 'ELK': 3, 'ELLIS': 2, 'ELLSWORTH': 2, 'FINNEY': 4, 'FORD': 3, 'FRANKLIN': 2, 'GEARY': 1, 'GOVE': 1, 'GRAHAM': 7, 'GRANT': 2, 'GRAY': 2, 'GREELEY': 1, 'GREENWOOD': 3, 'HAMILTON': 1, 'HARPER': 1, 'HARVEY': 0, 'HASKELL': 6, 'HODGEMAN': 5, 'JACKSON': 2, 'JEFFERSON': 3, 'JEWELL': 2, 'JOHNSON': 5, 'KEARNY': 3, 'KINGMAN': 1, 'KIOWA': 4, 'LABETTE': 2, 'LANE': 3, 'LEAVENWORTH': 0, 'LINCOLN': 5, 'LINN': 4, 'LOGAN': 5, 'LYON': 2, 'MARION': 1, 'MARSHALL': 5, 'MCPHERSON': 0, 'MEADE': 6, 'MIAMI': 3, 'MITCHELL': 0, 'MONTGOMERY': 1, 'MORRIS': 0, 'MORTON': 1, 'NEMAHA': 6, 'NEOSHO': 2, 'NESS': 2, 'NORTON': 4, 'OSAGE': 0, 'OSBORNE': 3, 'OTTAWA': 0, 'PAWNEE': 0, 'PHILLIPS': 1, 'POTTAWATOMIE': 1, 'PRATT': 3, 'RAWLINS': 2, 'RENO': 1, 'REPUBLIC': 0, 'RICE': 4, 'RILEY': 6, 'ROOKS': 2, 'RUSH': 5, 'RUSSELL': 2, 'SALINE': 0, 'SCOTT': 5, 'SEDGWICK': 0, 'SEWARD': 1, 'SHAWNEE': 0, 'SHERIDAN': 7, 'SHERMAN': 0, 'SMITH': 1, 'STAFFORD': 4, 'STANTON': 8, 'STEVENS': 1, 'SUMNER': 0, 'THOMAS': 8, 'TREGO': 5, 'WABAUNSEE': 5, 'WALLACE': 4, 'WASHINGTON': 4, 'WICHITA': 1, 'WILSON': 2, 'WOODSON': 3, 'WYANDOTTE': 15}\n"
     ]
    }
   ],
   "source": [
    "lengths = {key: len(value) for key, value in missing_years_by_county.items()}\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4755e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4219f",
   "metadata": {},
   "source": [
    "### <span style=color:blue> Add missing yield data for every County in Kansas state </span>\n",
    "#### <span style=color:blue> Manullay added missed yield data using 'linear interpolation' method. But 'WYANDOTTE',  'DONIPHAN' county, there are so many omitted yield data per year. Then we set the yield value as 0.\n",
    " </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40f0f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the data set with the omitted yield value added \n",
    "archives_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/'\n",
    "output_file = '105Counties_repaired_yield.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555f187",
   "metadata": {},
   "source": [
    "#### <span style=color:blue>Saving the csv I'm happy with in a designated place in my \"archives\" directory</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "020f70ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/KENSAS_winter_wheat_yield_data_3.csv'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/USDA-NASS--v01/'\n",
    "archives_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/'\n",
    "src_file = output_file # from preceding cell\n",
    "tgt_file = 'KENSAS_winter_wheat_yield_data_3.csv'\n",
    "\n",
    "shutil.copyfile(output_dir + src_file, archives_dir + tgt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58931598",
   "metadata": {},
   "source": [
    "#### <span style=color:blue>Projecting out the columns and records that I don't need for my ML learning table, and archiving that result, also. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1624c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state_name county_name  yield\n",
      "0  2022     KANSAS       ALLEN  53.50\n",
      "1  2021     KANSAS       ALLEN  54.90\n",
      "2  2020     KANSAS       ALLEN  49.70\n",
      "3  2019     KANSAS       ALLEN  44.50\n",
      "4  2018     KANSAS       ALLEN  43.65\n",
      "\n",
      "2101\n",
      "Empty DataFrame\n",
      "Columns: [year, state_name, county_name, yield]\n",
      "Index: []\n",
      "\n",
      "wrote file  /Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/d_KENSAS_WHEAT_3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "archives_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/'\n",
    "tgt_file = 'KENSAS_winter_wheat_yield_data_3.csv'\n",
    "\n",
    "df = pd.read_csv(archives_dir + tgt_file)\n",
    "# print(df.head())\n",
    "\n",
    "cols_to_keep = ['year','state_name','county_name','yield']\n",
    "dfml = df[cols_to_keep]\n",
    "\n",
    "print(dfml.head())\n",
    "print()\n",
    "print(dfml.shape[0])\n",
    "# Note: this particular df has 2050 rows\n",
    "\n",
    "# checking there are no null values for 'yield':\n",
    "print(dfml[dfml['yield'].isnull()].head())\n",
    "\n",
    "tgt_file_01 = 'd_KENSAS_WHEAT_3'\n",
    "dfml.to_csv(archives_dir + tgt_file_01, index=False)\n",
    "print('\\nwrote file ', archives_dir + tgt_file_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1da869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county_name\n",
      "BROWN        1057.5\n",
      "NEMAHA       1022.6\n",
      "MIAMI         962.8\n",
      "RICE          959.3\n",
      "CHEROKEE      951.5\n",
      "              ...  \n",
      "HAMILTON      632.7\n",
      "COMANCHE      632.7\n",
      "MORTON        618.1\n",
      "DONIPHAN      245.0\n",
      "WYANDOTTE     218.0\n",
      "Name: yield, Length: 105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sum_by_county = df.groupby('county_name')['yield'].sum()\n",
    "sum_by_county = sum_by_county.sort_values(ascending=False)\n",
    "print(sum_by_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b7d084b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2003    5327.00\n",
      "2016    5296.91\n",
      "2021    5148.80\n",
      "2017    5015.00\n",
      "2019    4712.15\n",
      "2020    4588.90\n",
      "2012    4445.45\n",
      "2018    4348.25\n",
      "2022    4326.00\n",
      "2010    4276.95\n",
      "2013    4224.34\n",
      "2009    4159.50\n",
      "2005    4051.00\n",
      "2015    4002.92\n",
      "2004    3920.00\n",
      "2008    3803.80\n",
      "2011    3688.50\n",
      "2006    3664.00\n",
      "2014    3577.48\n",
      "2007    3005.00\n",
      "Name: yield, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check total yield volume in Kansas state\n",
    "sum_by_year = df.groupby('year')['yield'].sum()\n",
    "sum_by_year = sum_by_year.sort_values(ascending=False)\n",
    "print(sum_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ad4dc123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALLEN': [], 'ANDERSON': [], 'ATCHISON': [], 'BARBER': [], 'BARTON': [], 'BOURBON': [], 'BROWN': [], 'BUTLER': [], 'CHASE': [], 'CHAUTAUQUA': [], 'CHEROKEE': [], 'CHEYENNE': [], 'CLARK': [], 'CLAY': [], 'CLOUD': [], 'COFFEY': [], 'COMANCHE': [], 'COWLEY': [], 'CRAWFORD': [], 'DECATUR': [], 'DICKINSON': [], 'DONIPHAN': [], 'DOUGLAS': [], 'EDWARDS': [], 'ELK': [], 'ELLIS': [], 'ELLSWORTH': [], 'FINNEY': [], 'FORD': [], 'FRANKLIN': [], 'GEARY': [], 'GOVE': [], 'GRAHAM': [], 'GRANT': [], 'GRAY': [], 'GREELEY': [], 'GREENWOOD': [], 'HAMILTON': [], 'HARPER': [], 'HARVEY': [], 'HASKELL': [], 'HODGEMAN': [], 'JACKSON': [], 'JEFFERSON': [], 'JEWELL': [], 'JOHNSON': [], 'KEARNY': [], 'KINGMAN': [], 'KIOWA': [], 'LABETTE': [], 'LANE': [], 'LEAVENWORTH': [], 'LINCOLN': [], 'LINN': [], 'LOGAN': [], 'LYON': [], 'MARION': [], 'MARSHALL': [], 'MCPHERSON': [], 'MEADE': [], 'MIAMI': [], 'MITCHELL': [], 'MONTGOMERY': [], 'MORRIS': [], 'MORTON': [], 'NEMAHA': [], 'NEOSHO': [], 'NESS': [], 'NORTON': [], 'OSAGE': [], 'OSBORNE': [], 'OTTAWA': [], 'PAWNEE': [], 'PHILLIPS': [], 'POTTAWATOMIE': [], 'PRATT': [], 'RAWLINS': [], 'RENO': [], 'REPUBLIC': [], 'RICE': [], 'RILEY': [], 'ROOKS': [], 'RUSH': [], 'RUSSELL': [], 'SALINE': [], 'SCOTT': [], 'SEDGWICK': [], 'SEWARD': [], 'SHAWNEE': [], 'SHERIDAN': [], 'SHERMAN': [], 'SMITH': [], 'STAFFORD': [], 'STANTON': [], 'STEVENS': [], 'SUMNER': [], 'THOMAS': [], 'TREGO': [], 'WABAUNSEE': [], 'WALLACE': [], 'WASHINGTON': [], 'WICHITA': [], 'WILSON': [], 'WOODSON': [], 'WYANDOTTE': []}\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# Checking our data after cleaning and wrangling our data \n",
    "county_years = df.groupby('county_name')['year'].agg(['min', 'max'])\n",
    "\n",
    "start_year = 2003\n",
    "end_year = 2022\n",
    "\n",
    "missing_years_by_county = {}\n",
    "for county, years in county_years.iterrows():\n",
    "\n",
    "    # Generate a list of all years between the start and end years for each county\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "\n",
    "    # Find any missing years for each county by comparing the list of all years with the available years in the data\n",
    "    missing_years_by_county[county] = list(set(all_years) - set(df[df['county_name'] == county]['year']))\n",
    "    \n",
    "print(missing_years_by_county)\n",
    "print(len(missing_years_by_county))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b66269",
   "metadata": {},
   "source": [
    "## Add data feature - Plant area data from USDA-NASS\n",
    "### <span style=color:blue>Get Planting progress values from USDA NASS </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bc45cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/USDA-NASS--v01/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e0c20c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quickstats.nass.usda.gov/api/api_GET/?key=CECCBC6B-9398-356F-9BCC-C326CBB2DFB4&source_desc=SURVEY&sector_desc=CROPS&group_desc%3DFIELD%20CROPS&commodity_desc=WHEAT&statisticcat_desc%3DAREA%20PLANTED&short_desc%3DWHEAT%2C%20WINTER%20-%20ACRES%20PLANTED&geographic_level=STATE&domain_desc=TOTAL&agg_level_desc=COUNTY&state_name=KANSAS&year__GE=2003&year__LE=2023&format=CSV\n",
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "parameters =    'source_desc=SURVEY' +  \\\n",
    "                '&sector_desc=CROPS' + \\\n",
    "                '&' + urllib.parse.quote('group_desc=FIELD CROPS') + \\\n",
    "                '&commodity_desc=WHEAT' + \\\n",
    "                '&' + urllib.parse.quote('statisticcat_desc=AREA PLANTED') + \\\n",
    "                '&' + urllib.parse.quote('short_desc=WHEAT, WINTER - ACRES PLANTED') + \\\n",
    "'&geographic_level=STATE' + \\\n",
    "                '&domain_desc=TOTAL' + \\\n",
    "                '&agg_level_desc=COUNTY' + \\\n",
    "                '&state_name=KANSAS' + \\\n",
    "                '&year__GE=2003' + \\\n",
    "                '&year__LE=2023' + \\\n",
    "                '&format=CSV'\n",
    "\n",
    "stats = c_usda_quick_stats()\n",
    "\n",
    "# holding this timestamp; we may used it to import the created csv file\n",
    "latest_curr_timestamp = curr_timestamp()\n",
    "filename = 'wheat_areaPlanted_data__' + latest_curr_timestamp + '.csv'\n",
    "\n",
    "# Including curr_timestamp() into file name to keep outputs separated during development/exploration\n",
    "stats.get_data(parameters, output_dir, 'wheat_areaPlanted_data__' + latest_curr_timestamp + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/USDA-NASS--v01/'\n",
    "\n",
    "df_areaPlant = pd.read_csv(output_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49394cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_county_names = ['OTHER COUNTIES', 'OTHER (COMBINED) COUNTIES']\n",
    "df_areaPlant = df_areaPlant[~df_areaPlant.county_name.isin(bad_county_names)]\n",
    "df_areaPlant = df_areaPlant.rename(columns={'Value': 'area_planted'})\n",
    "\n",
    "df_areaPlant = df_areaPlant[df_areaPlant['short_desc'].isin(['WHEAT, WINTER - ACRES PLANTED'])]\n",
    "\n",
    "output_file = 'repaired_wheat_areaPlanted_data__' + curr_timestamp() + '.csv'\n",
    "\n",
    "df_areaPlant.to_csv(output_dir + output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13107dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_areaPlant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_areaPlant = df_areaPlant[['state_name','county_name']].drop_duplicates()\n",
    "print(df2_areaPlant)\n",
    "# Total number of counties : 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'county' and get the minimum and maximum years for each county\n",
    "county_years = df_areaPlant.groupby('county_name')['year'].agg(['min', 'max'])\n",
    "\n",
    "start_year = 2003\n",
    "end_year = 2022\n",
    "\n",
    "# Iterate over each county and find missing years for each county\n",
    "missing_years_by_county = {}\n",
    "for county, years in county_years.iterrows():\n",
    "\n",
    "    # Generate a list of all years between the start and end years for each county\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "\n",
    "    # Find any missing years for each county by comparing the list of all years with the available years in the data\n",
    "    missing_years_by_county[county] = list(set(all_years) - set(df_areaPlant[df_areaPlant['county_name'] == county]['year']))\n",
    "    \n",
    "print(missing_years_by_county)\n",
    "print(len(missing_years_by_county))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2d76fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year state_name county_name area_planted\n",
      "10424  2022     KANSAS    CHEYENNE      103,500\n",
      "10425  2021     KANSAS    CHEYENNE      103,500\n",
      "10426  2020     KANSAS    CHEYENNE       95,000\n",
      "10427  2019     KANSAS    CHEYENNE      113,600\n",
      "10428  2018     KANSAS    CHEYENNE      116,200\n",
      "\n",
      "1817\n",
      "Empty DataFrame\n",
      "Columns: [year, state_name, county_name, area_planted]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "cols_to_keep = ['year','state_name','county_name','area_planted']\n",
    "df_areaP = df_areaPlant[cols_to_keep]\n",
    "\n",
    "print(df_areaP.head())\n",
    "print()\n",
    "print(df_areaP.shape[0])\n",
    "# Note: this particular df has 2050 rows\n",
    "\n",
    "# checking there are no null values for 'yield':\n",
    "print(df_areaP[df_areaP['area_planted'].isnull()].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e6737e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "archives_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/'\n",
    "tgt_file = 'KENSAS_winter_wheat_areaPlant_data.csv'\n",
    "df_areaP.to_csv(archives_dir + tgt_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "39ff1f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year state_name county_name  yield  area_planted\n",
      "0     2022     KANSAS       ALLEN  53.50         12100\n",
      "1     2021     KANSAS       ALLEN  54.90         13800\n",
      "2     2020     KANSAS       ALLEN  49.70             0\n",
      "3     2019     KANSAS       ALLEN  44.50          4300\n",
      "4     2018     KANSAS       ALLEN  43.65             0\n",
      "...    ...        ...         ...    ...           ...\n",
      "2096  2007     KANSAS   WYANDOTTE  20.00           600\n",
      "2097  2006     KANSAS   WYANDOTTE  43.00           400\n",
      "2098  2005     KANSAS   WYANDOTTE  43.00           400\n",
      "2099  2004     KANSAS   WYANDOTTE  49.00           800\n",
      "2100  2003     KANSAS   WYANDOTTE  63.00           400\n",
      "\n",
      "[2101 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import np\n",
    "\n",
    "merged_df = pd.merge(dfml, df_areaP[['year', 'county_name', 'area_planted']], on=['year', 'county_name'], how='left')\n",
    "# Rename the merged 'area_planted' column to avoid duplicates\n",
    "merged_df.rename(columns={'area_planted_y': 'area_planted'}, inplace=True)\n",
    "\n",
    "# Drop the unnecessary 'area_planted_x' column\n",
    "merged_df.drop(columns='area_planted_x', inplace=True)\n",
    "\n",
    "merged_df['area_planted'] = merged_df['area_planted'].str.replace(',', '').fillna(0).astype(np.int64)\n",
    "\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f0477d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "archives_dir = '/Users/jinholee/Desktop_local/2023_Spring_FoodSecurity/HW3/output/ML-ARCHIVES--v01/'\n",
    "tgt_file = 'KENSAS_winter_wheat_yield_plant_data.csv'\n",
    "\n",
    "merged_df.to_csv(archives_dir + tgt_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19def859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
